"""
Google News æ•°æ®æº
åŸºäº Google News RSS æä¾›å…è´¹çš„æ–°é—»æ•°æ®
"""

import asyncio
import logging
from typing import Dict, List, Any
from datetime import datetime, timedelta
import requests
import feedparser
from bs4 import BeautifulSoup
from urllib.parse import quote_plus

from .base_source import BaseDataSource, DataSourceType
from ..proxy_config import get_proxy_config

logger = logging.getLogger(__name__)

class GoogleNewsSource(BaseDataSource):
    """Google News æ•°æ®æº"""
    
    def __init__(self):
        super().__init__("GoogleNews", DataSourceType.NEWS)
        self.cache_timeout = 900  # 15åˆ†é’Ÿç¼“å­˜
        self.proxy_config = get_proxy_config()
        
        # è®¾ç½®ä¼šè¯
        self.session = self.proxy_config.setup_requests_session()
        if self.session is None:
            self.session = requests.Session()
        
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        logger.info("Google News æ•°æ®æºåˆå§‹åŒ–å®Œæˆ")
    
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            # ä½¿ç”¨æ›´ç®€å•å’Œç¨³å®šçš„ Google News RSS URL
            test_urls = [
                "https://news.google.com/rss?hl=en&gl=US&ceid=US:en",
                "https://news.google.com/rss/search?q=AAPL&hl=en&gl=US&ceid=US:en",
                "https://news.google.com/news/rss"
            ]
            
            for url in test_urls:
                try:
                    response = self.session.get(url, timeout=10)
                    if response.status_code == 200:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ RSS å†…å®¹
                        if '<?xml' in response.text or '<rss' in response.text:
                            logger.info(f"Google News å¥åº·æ£€æŸ¥æˆåŠŸï¼Œä½¿ç”¨ URL: {url}")
                            return True
                except Exception as e:
                    logger.debug(f"URL {url} æµ‹è¯•å¤±è´¥: {e}")
                    continue
            
            logger.warning("æ‰€æœ‰ Google News URL éƒ½æ— æ³•è®¿é—®")
            return False
            
        except Exception as e:
            logger.error(f"Google News å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    async def get_company_news(
        self, 
        symbol: str, 
        start_date: str, 
        end_date: str,
        limit: int = 20
    ) -> List[Dict[str, Any]]:
        """è·å–å…¬å¸æ–°é—»"""
        try:
            cache_key = f"news_{symbol}_{start_date}_{end_date}_{limit}"
            cached_data = self._get_cached_data(cache_key)
            if cached_data is not None:
                return cached_data
            
            # æ„å»ºæ™ºèƒ½æœç´¢æŸ¥è¯¢
            queries = self._build_smart_queries(symbol)
            
            all_articles = []
            
            for query in queries:
                try:
                    articles = await self._search_google_news(query, limit//len(queries) + 3)
                    all_articles.extend(articles)
                except Exception as e:
                    logger.warning(f"Google News æœç´¢å¤±è´¥ {query}: {e}")
                    continue
            
            # å»é‡å’Œæ’åº
            unique_articles = self._deduplicate_articles(all_articles)
            
            # æŒ‰æ—¶é—´è¿‡æ»¤
            filtered_articles = self._filter_by_date(unique_articles, start_date, end_date)
            
            # é™åˆ¶æ•°é‡
            result = filtered_articles[:limit]
            
            # ç¼“å­˜ç»“æœ
            self._cache_data(cache_key, result)
            
            return result
            
        except Exception as e:
            logger.error(f"è·å–å…¬å¸æ–°é—»å¤±è´¥ {symbol}: {e}")
            return []
    
    async def get_company_profile(
        self, 
        symbol: str, 
        detailed: bool = False
    ) -> Dict[str, Any]:
        """Google News ä¸æ”¯æŒå…¬å¸ä¿¡æ¯"""
        return {
            "error": "NOT_SUPPORTED",
            "message": "Google News æ•°æ®æºä¸æ”¯æŒå…¬å¸ä¿¡æ¯æŸ¥è¯¢",
            "suggestion": "è¯·ä½¿ç”¨ yfinance æˆ– alpha_vantage æ•°æ®æº"
        }
    
    async def is_supported(self, symbol: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ”¯æŒè¯¥è‚¡ç¥¨ä»£ç """
        # Google News æ”¯æŒä»»ä½•è‚¡ç¥¨ä»£ç çš„æ–°é—»æœç´¢
        return True
    
    async def get_rate_limit_info(self) -> Dict[str, Any]:
        """è·å–APIé™åˆ¶ä¿¡æ¯"""
        return {
            "source": "google_news",
            "daily_limit": -1,  # æ— é™åˆ¶
            "remaining_calls": -1,
            "reset_time": None,
            "is_free": True
        }
    
    async def _search_google_news(
        self, 
        query: str, 
        max_results: int = 10,
        language: str = "en",
        country: str = "US"
    ) -> List[Dict[str, Any]]:
        """æœç´¢ Google News"""
        try:
            encoded_query = quote_plus(query)
            url = f"https://news.google.com/rss/search?q={encoded_query}&hl={language}&gl={country}&ceid={country}:{language}"
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            # è§£æ RSS
            feed = feedparser.parse(response.content)
            
            articles = []
            for entry in feed.entries[:max_results]:
                try:
                    # è§£æå‘å¸ƒæ—¶é—´
                    pub_date = datetime.now()
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date = datetime(*entry.published_parsed[:6])
                    
                    # æ¸…ç†æ ‡é¢˜å’Œæè¿°
                    title = entry.title if hasattr(entry, 'title') else ""
                    description = entry.summary if hasattr(entry, 'summary') else ""
                    
                    # ç§»é™¤ HTML æ ‡ç­¾
                    if title:
                        title = BeautifulSoup(title, 'html.parser').get_text()
                    if description:
                        description = BeautifulSoup(description, 'html.parser').get_text()
                    
                    article = {
                        "id": f"google_news_{hash(entry.link if hasattr(entry, 'link') else title)}",
                        "headline": title,
                        "summary": description,
                        "source": entry.source.title if hasattr(entry, 'source') and hasattr(entry.source, 'title') else "Google News",
                        "url": entry.link if hasattr(entry, 'link') else "",
                        "datetime": pub_date.strftime("%Y-%m-%d %H:%M:%S"),
                        "related": query,
                        "image": "",
                        "category": "business",
                        "data_source": "google_news"
                    }
                    
                    articles.append(article)
                    
                except Exception as e:
                    logger.warning(f"å¤„ç†æ–°é—»æ¡ç›®å¤±è´¥: {e}")
                    continue
            
            return articles
            
        except Exception as e:
            logger.error(f"Google News æœç´¢å¤±è´¥: {e}")
            return []
    
    def _deduplicate_articles(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡æ–°é—»æ–‡ç« """
        seen_urls = set()
        seen_titles = set()
        unique_articles = []
        
        for article in articles:
            url = article.get('url', '')
            title = article.get('headline', '')
            
            # åŸºäºURLå’Œæ ‡é¢˜å»é‡
            if url and url not in seen_urls and title not in seen_titles:
                seen_urls.add(url)
                seen_titles.add(title)
                unique_articles.append(article)
        
        return unique_articles
    
    def _build_smart_queries(self, symbol: str) -> List[str]:
        """æ„å»ºæ™ºèƒ½æœç´¢æŸ¥è¯¢ï¼Œå‡å°‘æ··æ·†"""
        queries = []
        
        # æ£€æµ‹è‚¡ç¥¨äº¤æ˜“æ‰€å’Œæ ¼å¼
        if '.HK' in symbol.upper():
            # é¦™æ¸¯è‚¡ç¥¨ - ä½¿ç”¨æ›´ç²¾ç¡®çš„æŸ¥è¯¢
            base_symbol = symbol.split('.')[0]
            queries.extend([
                f'"{symbol}" stock Hong Kong',  # ç²¾ç¡®åŒ¹é…å¸¦å¼•å·
                f'"{base_symbol}.HK" earnings',
                f'"{symbol}" HKEX',  # é¦™æ¸¯äº¤æ˜“æ‰€
                f'{base_symbol} HK stock news',
                f'Hong Kong stock {base_symbol}'
            ])
        elif '.SS' in symbol.upper() or '.SZ' in symbol.upper():
            # ä¸­å›½Aè‚¡
            queries.extend([
                f'"{symbol}" stock Shanghai',
                f'"{symbol}" stock Shenzhen', 
                f'"{symbol}" Aè‚¡',
                f'{symbol} è‚¡ç¥¨'
            ])
        elif len(symbol) == 4 and symbol.isdigit():
            # å¯èƒ½æ˜¯æ—¥æœ¬è‚¡ç¥¨ä»£ç ï¼Œéœ€è¦æ˜ç¡®æŒ‡å®š
            queries.extend([
                f'"{symbol}" stock Japan TSE',
                f'TSE:{symbol}',
                f'Tokyo stock {symbol}'
            ])
        else:
            # ç¾è‚¡æˆ–å…¶ä»–
            queries.extend([
                f'"{symbol}" stock',
                f'"{symbol}" earnings',
                f'"{symbol}" news',
                f'{symbol} stock market'
            ])
        
        # é™åˆ¶æŸ¥è¯¢æ•°é‡é¿å…è¿‡åº¦æœç´¢
        return queries[:4]
    
    def _filter_by_date(
        self, 
        articles: List[Dict[str, Any]], 
        start_date: str, 
        end_date: str
    ) -> List[Dict[str, Any]]:
        """æŒ‰æ—¥æœŸè¿‡æ»¤æ–°é—»ï¼ˆæ”¾å®½æ—¥æœŸé™åˆ¶ï¼‰"""
        try:
            start_dt = datetime.strptime(start_date, "%Y-%m-%d")
            end_dt = datetime.strptime(end_date, "%Y-%m-%d") + timedelta(days=1)  # åŒ…å«ç»“æŸæ—¥æœŸ
            
            # ğŸ“ˆ Google News ç»å¸¸è¿”å›è¾ƒæ—§çš„æ–°é—»ï¼Œå› æ­¤æ”¾å®½æ—¥æœŸèŒƒå›´
            # å¦‚æœè¯·æ±‚èŒƒå›´å°äº7å¤©ï¼Œåˆ™æ‰©å±•åˆ°30å¤©å‰
            date_range_days = (end_dt - start_dt).days
            if date_range_days <= 7:
                start_dt = start_dt - timedelta(days=30)
                logger.info(f"æ”¾å®½æ—¥æœŸè¿‡æ»¤èŒƒå›´åˆ° {start_dt.strftime('%Y-%m-%d')} - {end_dt.strftime('%Y-%m-%d')}")
            
            filtered = []
            no_date_articles = []
            
            for article in articles:
                try:
                    article_dt = datetime.strptime(article['datetime'], "%Y-%m-%d %H:%M:%S")
                    if start_dt <= article_dt <= end_dt:
                        filtered.append(article)
                    else:
                        logger.debug(f"æ–‡ç« æ—¥æœŸ {article['datetime']} è¶…å‡ºèŒƒå›´ {start_dt} - {end_dt}")
                except (ValueError, KeyError):
                    # æ—¥æœŸè§£æå¤±è´¥ï¼Œä¿ç•™æ–‡ç« ä½†æ”¾åœ¨åé¢
                    logger.debug(f"æ— æ³•è§£ææ–‡ç« æ—¥æœŸ: {article.get('datetime', 'N/A')}")
                    no_date_articles.append(article)
            
            # å¦‚æœä¸¥æ ¼è¿‡æ»¤åæ²¡æœ‰ç»“æœï¼ŒåŒ…å«ä¸€äº›æ— æ—¥æœŸçš„æ–‡ç« 
            if not filtered and no_date_articles:
                logger.info("ä¸¥æ ¼æ—¥æœŸè¿‡æ»¤æ— ç»“æœï¼ŒåŒ…å«éƒ¨åˆ†æ— æ—¥æœŸæ–‡ç« ")
                filtered.extend(no_date_articles[:5])  # æœ€å¤šåŒ…å«5ç¯‡æ— æ—¥æœŸæ–‡ç« 
            
            # æŒ‰æ—¶é—´å€’åºæ’åº
            filtered.sort(key=lambda x: x.get('datetime', ''), reverse=True)
            return filtered
            
        except Exception as e:
            logger.warning(f"æ—¥æœŸè¿‡æ»¤å¤±è´¥: {e}ï¼Œè¿”å›æ‰€æœ‰æ–‡ç« ")
            return articles
    
    def __str__(self) -> str:
        return "GoogleNewsSource(å…è´¹æ— é™åˆ¶)"