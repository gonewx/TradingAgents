"""
ç¤¾äº¤åª’ä½“æƒ…ç»ªåˆ†ææœåŠ¡æ¨¡å—
"""

import asyncio
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import os
import re
from collections import Counter

logger = logging.getLogger(__name__)

class SocialSentimentService:
    """ç¤¾äº¤åª’ä½“æƒ…ç»ªåˆ†ææœåŠ¡"""
    
    def __init__(self):
        self.cache = {}
        # ä»ç¯å¢ƒå˜é‡è¯»å–æ–°é—»ç¼“å­˜è¶…æ—¶ï¼Œé»˜è®¤15åˆ†é’Ÿ
        self.cache_timeout = int(os.getenv('NEWS_CACHE_TTL', '900'))
        
        # Reddit APIé…ç½®
        self.reddit_client_id = os.getenv("REDDIT_CLIENT_ID")
        self.reddit_secret = os.getenv("REDDIT_SECRET")
        
        # æƒ…ç»ªåˆ†æè¯å…¸
        self.bullish_terms = {
            'moon', 'rocket', 'diamond hands', 'hodl', 'buy the dip', 'to the moon',
            'bullish', 'calls', 'yolo', 'stonks', 'apes', 'tendies', 'green',
            'pump', 'squeeze', 'gains', 'lambo', 'bull', 'strong buy'
        }
        
        self.bearish_terms = {
            'crash', 'dump', 'red', 'puts', 'short', 'sell', 'drop', 'fall',
            'bearish', 'panic', 'loss', 'rip', 'dead', 'bag holder', 'bear',
            'correction', 'bubble', 'overvalued'
        }
        
        # å¸¸è§è‚¡ç¥¨æœ¯è¯­
        self.stock_terms = {
            'dd', 'due diligence', 'earnings', 'options', 'calls', 'puts',
            'strike', 'expiry', 'volume', 'float', 'market cap', 'pe ratio'
        }
        
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æ£€æŸ¥é…ç½®æ˜¯å¦å®Œæ•´
            return True  # å³ä½¿æ²¡æœ‰APIå¯†é’¥ä¹Ÿå¯ä»¥æä¾›åŸºæœ¬åŠŸèƒ½
        except Exception as e:
            logger.error(f"Social sentiment health check failed: {e}")
            return False
    
    async def get_reddit_sentiment(
        self, 
        ticker: str, 
        subreddit: str = "wallstreetbets"
    ) -> Dict[str, Any]:
        """åˆ†æRedditæƒ…ç»ª"""
        try:
            cache_key = f"reddit_{ticker}_{subreddit}"
            if self._is_cache_valid(cache_key):
                return self.cache[cache_key]["data"]
            
            # æ¨¡æ‹ŸRedditæ•°æ®è·å–ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦ä½¿ç”¨PRAWåº“ï¼‰
            posts = await self._fetch_reddit_posts(ticker, subreddit)
            
            if not posts:
                return {
                    "ticker": ticker,
                    "subreddit": subreddit,
                    "sentiment_score": 0.5,
                    "bullish_ratio": 0.5,
                    "post_count": 0,
                    "comment_count": 0,
                    "trending_score": 0,
                    "sentiment_distribution": {"bullish": 0, "neutral": 0, "bearish": 0},
                    "timestamp": datetime.now().isoformat()
                }
            
            # åˆ†ææƒ…ç»ª
            sentiment_analysis = self._analyze_reddit_sentiment(posts, ticker)
            
            result = {
                "ticker": ticker,
                "subreddit": subreddit,
                "sentiment_score": sentiment_analysis["sentiment_score"],
                "bullish_ratio": sentiment_analysis["bullish_ratio"],
                "post_count": len(posts),
                "comment_count": sum(post.get("num_comments", 0) for post in posts),
                "trending_score": sentiment_analysis["trending_score"],
                "sentiment_distribution": sentiment_analysis["distribution"],
                "top_posts": posts[:5],  # å‰5ä¸ªçƒ­é—¨å¸–å­
                "popular_terms": sentiment_analysis["popular_terms"],
                "timestamp": datetime.now().isoformat()
            }
            
            # ç¼“å­˜ç»“æœ
            self._cache_data(cache_key, result)
            
            return result
            
        except Exception as e:
            logger.error(f"Failed to analyze Reddit sentiment for {ticker}: {e}")
            return {
                "ticker": ticker,
                "subreddit": subreddit,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def get_trending_tickers(self, source: str = "all") -> List[Dict[str, Any]]:
        """è·å–çƒ­é—¨è‚¡ç¥¨"""
        try:
            cache_key = f"trending_{source}"
            if self._is_cache_valid(cache_key):
                return self.cache[cache_key]["data"]
            
            trending_data = []
            
            if source in ["reddit", "all"]:
                reddit_trending = await self._get_reddit_trending()
                trending_data.extend(reddit_trending)
            
            if source in ["twitter", "all"]:
                twitter_trending = await self._get_twitter_trending()
                trending_data.extend(twitter_trending)
            
            # åˆå¹¶å’Œæ’åº
            merged_tickers = self._merge_trending_data(trending_data)
            
            # ç¼“å­˜ç»“æœ
            self._cache_data(cache_key, merged_tickers)
            
            return merged_tickers
            
        except Exception as e:
            logger.error(f"Failed to get trending tickers: {e}")
            return [{"error": str(e)}]
    
    async def _fetch_reddit_posts(self, ticker: str, subreddit: str) -> List[Dict[str, Any]]:
        """è·å–Redditå¸–å­ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰"""
        try:
            # å®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨PRAWåº“è¿æ¥Reddit API
            # è¿™é‡Œæä¾›æ¨¡æ‹Ÿæ•°æ®ç”¨äºæ¼”ç¤º
            
            sample_posts = [
                {
                    "title": f"{ticker} DD - Why this is going to moon ğŸš€",
                    "score": 1250,
                    "num_comments": 300,
                    "created_utc": datetime.now().timestamp(),
                    "selftext": f"Deep dive analysis on {ticker}. Strong fundamentals, great management team.",
                    "url": f"https://reddit.com/r/{subreddit}/sample1",
                    "author": "sample_user1"
                },
                {
                    "title": f"YOLO {ticker} calls ğŸ’ğŸ™Œ",
                    "score": 800,
                    "num_comments": 150,
                    "created_utc": (datetime.now() - timedelta(hours=2)).timestamp(),
                    "selftext": f"All in on {ticker} options. Diamond hands!",
                    "url": f"https://reddit.com/r/{subreddit}/sample2",
                    "author": "sample_user2"
                },
                {
                    "title": f"{ticker} earnings play - thoughts?",
                    "score": 450,
                    "num_comments": 80,
                    "created_utc": (datetime.now() - timedelta(hours=4)).timestamp(),
                    "selftext": f"Thinking about playing {ticker} earnings. What do you think?",
                    "url": f"https://reddit.com/r/{subreddit}/sample3",
                    "author": "sample_user3"
                }
            ]
            
            return sample_posts
            
        except Exception as e:
            logger.warning(f"Failed to fetch Reddit posts: {e}")
            return []
    
    def _analyze_reddit_sentiment(self, posts: List[Dict], ticker: str) -> Dict[str, Any]:
        """åˆ†æRedditå¸–å­æƒ…ç»ª"""
        try:
            bullish_count = 0
            bearish_count = 0
            neutral_count = 0
            total_score = 0
            all_text = ""
            
            for post in posts:
                title = post.get("title", "").lower()
                content = post.get("selftext", "").lower()
                text = f"{title} {content}"
                all_text += f" {text}"
                
                # è®¡ç®—æƒ…ç»ªå¾—åˆ†
                bullish_words = sum(1 for term in self.bullish_terms if term in text)
                bearish_words = sum(1 for term in self.bearish_terms if term in text)
                
                post_score = post.get("score", 0)
                total_score += post_score
                
                # æ ¹æ®å…³é”®è¯å’Œå¾—åˆ†åˆ¤æ–­æƒ…ç»ª
                if bullish_words > bearish_words:
                    bullish_count += 1
                elif bearish_words > bullish_words:
                    bearish_count += 1
                else:
                    neutral_count += 1
            
            total_posts = len(posts)
            if total_posts == 0:
                return {
                    "sentiment_score": 0.5,
                    "bullish_ratio": 0.5,
                    "trending_score": 0,
                    "distribution": {"bullish": 0, "neutral": 0, "bearish": 0},
                    "popular_terms": []
                }
            
            bullish_ratio = bullish_count / total_posts
            sentiment_score = (bullish_count + 0.5 * neutral_count) / total_posts
            trending_score = total_score / total_posts  # å¹³å‡å¾—åˆ†ä½œä¸ºçƒ­åº¦æŒ‡æ ‡
            
            # æå–çƒ­é—¨è¯æ±‡
            popular_terms = self._extract_popular_terms(all_text)
            
            return {
                "sentiment_score": sentiment_score,
                "bullish_ratio": bullish_ratio,
                "trending_score": trending_score,
                "distribution": {
                    "bullish": bullish_count,
                    "neutral": neutral_count,
                    "bearish": bearish_count
                },
                "popular_terms": popular_terms
            }
            
        except Exception as e:
            logger.error(f"Failed to analyze Reddit sentiment: {e}")
            return {
                "sentiment_score": 0.5,
                "bullish_ratio": 0.5,
                "trending_score": 0,
                "distribution": {"bullish": 0, "neutral": 0, "bearish": 0},
                "popular_terms": []
            }
    
    async def _get_reddit_trending(self) -> List[Dict[str, Any]]:
        """è·å–Redditçƒ­é—¨è‚¡ç¥¨"""
        try:
            # æ¨¡æ‹Ÿçƒ­é—¨è‚¡ç¥¨æ•°æ®
            trending_tickers = [
                {"ticker": "AAPL", "mentions": 1500, "sentiment": 0.75, "source": "reddit"},
                {"ticker": "TSLA", "mentions": 1200, "sentiment": 0.85, "source": "reddit"},
                {"ticker": "GME", "mentions": 800, "sentiment": 0.90, "source": "reddit"},
                {"ticker": "NVDA", "mentions": 600, "sentiment": 0.70, "source": "reddit"},
                {"ticker": "AMD", "mentions": 450, "sentiment": 0.65, "source": "reddit"}
            ]
            
            return trending_tickers
            
        except Exception as e:
            logger.warning(f"Failed to get Reddit trending: {e}")
            return []
    
    async def _get_twitter_trending(self) -> List[Dict[str, Any]]:
        """è·å–Twitterçƒ­é—¨è‚¡ç¥¨"""
        try:
            # æ¨¡æ‹ŸTwitteræ•°æ®
            trending_tickers = [
                {"ticker": "AAPL", "mentions": 800, "sentiment": 0.60, "source": "twitter"},
                {"ticker": "MSFT", "mentions": 600, "sentiment": 0.70, "source": "twitter"},
                {"ticker": "GOOGL", "mentions": 400, "sentiment": 0.65, "source": "twitter"},
                {"ticker": "META", "mentions": 350, "sentiment": 0.55, "source": "twitter"},
                {"ticker": "AMZN", "mentions": 300, "sentiment": 0.60, "source": "twitter"}
            ]
            
            return trending_tickers
            
        except Exception as e:
            logger.warning(f"Failed to get Twitter trending: {e}")
            return []
    
    def _merge_trending_data(self, trending_data: List[Dict]) -> List[Dict[str, Any]]:
        """åˆå¹¶å¤šä¸ªæ¥æºçš„çƒ­é—¨æ•°æ®"""
        try:
            ticker_data = {}
            
            for item in trending_data:
                ticker = item["ticker"]
                if ticker not in ticker_data:
                    ticker_data[ticker] = {
                        "ticker": ticker,
                        "total_mentions": 0,
                        "weighted_sentiment": 0,
                        "sources": []
                    }
                
                ticker_data[ticker]["total_mentions"] += item["mentions"]
                ticker_data[ticker]["weighted_sentiment"] += item["sentiment"] * item["mentions"]
                ticker_data[ticker]["sources"].append(item["source"])
            
            # è®¡ç®—åŠ æƒå¹³å‡æƒ…ç»ª
            for ticker_info in ticker_data.values():
                if ticker_info["total_mentions"] > 0:
                    ticker_info["avg_sentiment"] = ticker_info["weighted_sentiment"] / ticker_info["total_mentions"]
                else:
                    ticker_info["avg_sentiment"] = 0.5
                
                # æ¸…ç†ä¸´æ—¶å­—æ®µ
                del ticker_info["weighted_sentiment"]
            
            # æŒ‰æåŠæ¬¡æ•°æ’åº
            sorted_tickers = sorted(
                ticker_data.values(),
                key=lambda x: x["total_mentions"],
                reverse=True
            )
            
            return sorted_tickers[:20]  # è¿”å›å‰20ä¸ª
            
        except Exception as e:
            logger.error(f"Failed to merge trending data: {e}")
            return []
    
    def _extract_popular_terms(self, text: str) -> List[str]:
        """æå–çƒ­é—¨è¯æ±‡"""
        try:
            # æ¸…ç†æ–‡æœ¬
            text = re.sub(r'[^a-zA-Z0-9\s]', ' ', text.lower())
            words = text.split()
            
            # è¿‡æ»¤å¸¸è§è¯
            stop_words = {
                'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to',
                'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be',
                'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will',
                'would', 'could', 'should', 'may', 'might', 'can', 'this',
                'that', 'these', 'those'
            }
            
            # è¿‡æ»¤å¹¶ç»Ÿè®¡è¯é¢‘
            filtered_words = [
                word for word in words 
                if len(word) > 2 and word not in stop_words
            ]
            
            word_counts = Counter(filtered_words)
            
            # è¿”å›å‰10ä¸ªé«˜é¢‘è¯
            return [word for word, count in word_counts.most_common(10)]
            
        except Exception as e:
            logger.warning(f"Failed to extract popular terms: {e}")
            return []
    
    def _is_cache_valid(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if key not in self.cache:
            return False
        
        cache_time = self.cache[key]["timestamp"]
        return (datetime.now() - cache_time).seconds < self.cache_timeout
    
    def _cache_data(self, key: str, data: Any) -> None:
        """ç¼“å­˜æ•°æ®"""
        self.cache[key] = {
            "data": data,
            "timestamp": datetime.now()
        }